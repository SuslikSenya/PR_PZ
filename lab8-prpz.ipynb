{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13869302,"sourceType":"datasetVersion","datasetId":8836676}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install jiwer textdistance pyspellchecker","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T21:03:00.587361Z","iopub.execute_input":"2025-11-25T21:03:00.587834Z","iopub.status.idle":"2025-11-25T21:03:07.496182Z","shell.execute_reply.started":"2025-11-25T21:03:00.587805Z","shell.execute_reply":"2025-11-25T21:03:07.495331Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting textdistance\n  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\nCollecting pyspellchecker\n  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\nDownloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.14.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: textdistance, rapidfuzz, pyspellchecker, jiwer\nSuccessfully installed jiwer-4.0.0 pyspellchecker-0.8.3 rapidfuzz-3.14.3 textdistance-4.6.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nimport re\nfrom collections import Counter\nfrom typing import List, Tuple, Set\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom jiwer import wer, cer\nimport textdistance\nfrom spellchecker import SpellChecker\n\n\nBATCH_SIZE = 16\nEPOCHS = 10\nLEARNING_RATE = 3e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDATA_DIR = \"/kaggle/input/lab8-dataset/LJSpeech-1.1\"\nNUM_WORKERS = 0\nSR = 16000\nN_MELS = 128\nN_FFT = 512\nHOP_LENGTH = 160\nN_MFCC = 13\n\n\ndef download_ljspeech_if_missing(data_path: str = DATA_DIR) -> str:\n    \"\"\"\n    If dataset path doesn't exist, try downloading LJSpeech (if permitted).\n    Returns path to dataset folder.\n    \"\"\"\n    if os.path.exists(data_path):\n        return data_path\n\n    print(\"Dataset folder not found. Attempting to download LJSpeech (best-effort)...\")\n    if os.system(\"which wget > /dev/null\") == 0:\n        os.system(\"wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2 -q\")\n        os.system(\"tar -xjf LJSpeech-1.1.tar.bz2\")\n        if os.path.exists(\"LJSpeech-1.1\"):\n            return \"LJSpeech-1.1\"\n    raise FileNotFoundError(\n        f\"LJSpeech dataset not found at '{data_path}'. \"\n        \"Please download LJSpeech and set DATA_DIR accordingly.\"\n    )\n\n\ndef load_ljspeech_metadata(data_path: str) -> pd.DataFrame:\n    \"\"\"\n    Load metadata.csv and return DataFrame with columns: audio_path, text\n    \"\"\"\n    metadata_path = os.path.join(data_path, \"metadata.csv\")\n    if not os.path.exists(metadata_path):\n        raise FileNotFoundError(f\"metadata.csv not found at {metadata_path}\")\n\n    rows = []\n    with open(metadata_path, \"r\", encoding=\"utf-8\") as fh:\n        for line in fh:\n            parts = line.strip().split(\"|\")\n            if len(parts) >= 3:\n                fname, _, text = parts[:3]\n                audio_path = os.path.join(data_path, \"wavs\", f\"{fname}.wav\")\n                if os.path.exists(audio_path):\n                    rows.append({\"audio_path\": audio_path, \"text\": text})\n    df = pd.DataFrame(rows)\n    print(f\"Loaded {len(df)} audio files from LJSpeech metadata.\")\n    return df\n\n\nclass AudioProcessor:\n    \"\"\"\n    Loads audio and computes normalized log-mel spectrogram frames (time x n_mels).\n    \"\"\"\n\n    def __init__(self, sr: int = SR, n_mels: int = N_MELS, n_fft: int = N_FFT, hop_length: int = HOP_LENGTH):\n        self.sr = sr\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n\n    def load(self, path: str) -> np.ndarray:\n        audio, sr = librosa.load(path, sr=self.sr)\n        return audio\n\n    def extract_log_mel(self, audio: np.ndarray) -> np.ndarray:\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop_length, n_mels=self.n_mels\n        )\n        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n        return log_mel.T\n\n    def process(self, path: str) -> np.ndarray:\n        audio = self.load(path)\n        return self.extract_log_mel(audio)\n\n\nclass TextTransform:\n    \"\"\"\n    Character-level mapping. Keeps space and apostrophe and a-z lowercase.\n    Adds a blank index at the end for CTC.\n    \"\"\"\n\n    def __init__(self):\n        self.chars = [\" \", \"'\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}\n        self.idx_to_char = {i: c for i, c in enumerate(self.chars)}\n        self.blank_idx = len(self.chars)\n\n    def text_to_indices(self, text: str) -> List[int]:\n        text = text.lower()\n        indices = [self.char_to_idx.get(ch, self.char_to_idx[\" \"]) for ch in text if ch in self.char_to_idx]\n        return indices\n\n    def indices_to_text(self, indices: List[int]) -> str:\n        return \"\".join(self.idx_to_char.get(i, \"\") for i in indices)\n\n    def vocab_size(self) -> int:\n        return len(self.chars) + 1\n\n\nclass LJSpeechDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, audio_proc: AudioProcessor, text_transform: TextTransform):\n        self.df = df.reset_index(drop=True)\n        self.audio_proc = audio_proc\n        self.text_transform = text_transform\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        features = self.audio_proc.process(row[\"audio_path\"])\n        text_indices = self.text_transform.text_to_indices(row[\"text\"])\n        return torch.FloatTensor(features), torch.LongTensor(text_indices), features.shape[0], len(text_indices)\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Pads feature sequences (time x n_mels) and text index sequences.\n    Returns:\n      features_padded: (batch, max_time, n_mels)\n      texts_padded: (batch, max_text_len)\n      feature_lengths: (batch,)\n      text_lengths: (batch,)\n    \"\"\"\n    feats, texts, feat_lens, text_lens = zip(*batch)\n    feats_padded = pad_sequence(feats, batch_first=True)\n    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n    return feats_padded, texts_padded, torch.LongTensor(feat_lens), torch.LongTensor(text_lens)\n\n\nclass DeepSpeech2Like(nn.Module):\n    def __init__(self, n_mels: int, n_classes: int, n_rnn_layers: int = 5, rnn_hidden: int = 512, dropout: float = 0.1):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 5)),\n            nn.BatchNorm2d(32),\n            nn.Hardtanh(0, 20, inplace=True),\n            nn.Dropout(dropout),\n            nn.Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1), padding=(10, 5)),\n            nn.BatchNorm2d(32),\n            nn.Hardtanh(0, 20, inplace=True),\n            nn.Dropout(dropout),\n        )\n\n        self._conv_out_size = self._estimate_conv_output(n_mels)\n\n        self.rnn_layers = nn.ModuleList()\n        for i in range(n_rnn_layers):\n            input_size = self._conv_out_size if i == 0 else rnn_hidden * 2\n            self.rnn_layers.append(nn.GRU(input_size, rnn_hidden, num_layers=1, bidirectional=True, batch_first=True))\n\n        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(rnn_hidden * 2) for _ in range(n_rnn_layers)])\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(rnn_hidden * 2, n_classes)\n\n    def _estimate_conv_output(self, n_mels: int) -> int:\n        with torch.no_grad():\n            x = torch.zeros(1, 1, 100, n_mels)\n            x = self.conv(x)\n            b, c, t, f = x.size()\n            return c * f\n\n    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        x: (batch, time, n_mels)\n        lengths: (batch,) raw frame lengths (time before padding)\n        Returns:\n          log_probs: (time, batch, n_classes)\n          output_lengths: (batch,)\n        \"\"\"\n        batch_size = x.size(0)\n        x = x.unsqueeze(1)\n        x = self.conv(x)\n        b, channels, time, features = x.size()\n        x = x.permute(0, 2, 1, 3).contiguous()\n        x = x.view(batch_size, time, channels * features)\n\n        output_lengths = (lengths.float() / 4.0).ceil().long()\n        for i, rnn in enumerate(self.rnn_layers):\n            x_packed = pack_padded_sequence(x, output_lengths.cpu(), batch_first=True, enforce_sorted=False)\n            x_packed, _ = rnn(x_packed)\n            x, _ = pad_packed_sequence(x_packed, batch_first=True)\n\n            x = x.transpose(1, 2)\n            x = self.batch_norms[i](x)\n            x = x.transpose(1, 2)\n            x = self.dropout(x)\n\n        x = self.fc(x)\n        x = nn.functional.log_softmax(x, dim=-1)\n        x = x.transpose(0, 1)\n        return x, output_lengths\n\n\ndef train_network(\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    text_transform: TextTransform,\n    epochs: int = EPOCHS,\n    lr: float = LEARNING_RATE,\n    device: str = DEVICE,\n):\n    model = model.to(device)\n    criterion = nn.CTCLoss(blank=text_transform.blank_idx, zero_infinity=True)\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, verbose=True)\n\n    best_val = float(\"inf\")\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n        for batch_idx, (feats, texts, feat_lens, text_lens) in enumerate(train_loader, 1):\n            feats = feats.to(device)\n            texts = texts.to(device)\n            feat_lens = feat_lens.to(device)\n            text_lens = text_lens.to(device)\n\n            optimizer.zero_grad()\n            outputs, out_lens = model(feats, feat_lens)\n\n            loss = criterion(outputs, texts, out_lens, text_lens)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n            optimizer.step()\n\n            running_loss += loss.item()\n            if batch_idx % 50 == 0:\n                print(f\"Epoch {epoch}/{epochs} - Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n\n        avg_train = running_loss / max(1, len(train_loader))\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for feats, texts, feat_lens, text_lens in val_loader:\n                feats = feats.to(device)\n                texts = texts.to(device)\n                feat_lens = feat_lens.to(device)\n                text_lens = text_lens.to(device)\n                outputs, out_lens = model(feats, feat_lens)\n                loss = criterion(outputs, texts, out_lens, text_lens)\n                val_loss += loss.item()\n        avg_val = val_loss / max(1, len(val_loader))\n\n        print(f\"\\nEpoch {epoch} summary:\")\n        print(f\"  Train Loss: {avg_train:.4f}\")\n        print(f\"  Val   Loss: {avg_val:.4f}\")\n\n        scheduler.step(avg_val)\n\n        if avg_val < best_val:\n            best_val = avg_val\n            torch.save(model.state_dict(), \"deepspeech2_best.pth\")\n            print(\"Saved best model to deepspeech2_best.pth\")\n\n        print(\"-\" * 60)\n\n    return model\n\n\nclass GreedyDecoder:\n    def __init__(self, text_transform: TextTransform):\n        self.text_transform = text_transform\n        self.blank = text_transform.blank_idx\n\n    def decode(self, log_probs: torch.Tensor) -> List[str]:\n        \"\"\"\n        Greedy decode from model log probabilities.\n\n        log_probs: (time, batch, n_classes)\n        returns: list of decoded strings length=batch\n        \"\"\"\n        argmax = torch.argmax(log_probs, dim=-1)\n        batch_size = argmax.size(1)\n        decoded = []\n        for b in range(batch_size):\n            indices = argmax[:, b].cpu().numpy().tolist()\n            prev = None\n            chars = []\n            for idx in indices:\n                if idx != prev and idx != self.blank:\n                    if idx < len(self.text_transform.idx_to_char):\n                        chars.append(self.text_transform.idx_to_char[idx])\n                prev = idx\n            decoded.append(\"\".join(chars))\n        return decoded\n\n\ndef test_network(model: nn.Module, test_loader: DataLoader, text_transform: TextTransform, device: str = DEVICE):\n    model = model.to(device)\n    model.eval()\n    decoder = GreedyDecoder(text_transform)\n    predictions = []\n    targets = []\n\n    with torch.no_grad():\n        for feats, texts, feat_lens, text_lens in test_loader:\n            feats = feats.to(device)\n            outputs, out_lens = model(feats, feat_lens)\n            batch_preds = decoder.decode(outputs)\n            for i in range(texts.size(0)):\n                target_idx = texts[i, : text_lens[i]].cpu().numpy().tolist()\n                targets.append(text_transform.indices_to_text(target_idx))\n            predictions.extend(batch_preds)\n\n    wer_score = wer(targets, predictions)\n    cer_score = cer(targets, predictions)\n\n    print(\"\\nTest results:\")\n    print(f\"  WER: {wer_score * 100:.2f}%\")\n    print(f\"  CER: {cer_score * 100:.2f}%\")\n\n    print(\"\\nExample predictions:\")\n    for i in range(min(5, len(predictions))):\n        print(f\"\\nTarget    : {targets[i]}\")\n        print(f\"Prediction: {predictions[i]}\")\n\n    return predictions, targets, wer_score\n\n\nclass TextCorrector:\n    \"\"\"\n    Several light-weight post-processing methods:\n      - SpellChecker (pyspellchecker)\n      - Edit-distance matching against vocabulary\n      - Frequency-based correction using training corpus counts\n      - Regex-based cleanup\n    \"\"\"\n\n    def __init__(self):\n        self.spell = SpellChecker()\n        self.word_freq = Counter()\n\n    def fit_corpus(self, texts: List[str]):\n        for t in texts:\n            words = t.lower().split()\n            self.word_freq.update(words)\n\n    def spell_check(self, text: str) -> str:\n        words = text.split()\n        out = []\n        for w in words:\n            if w in self.spell:\n                out.append(w)\n            else:\n                c = self.spell.correction(w)\n                out.append(c if c else w)\n        return \" \".join(out)\n\n    def edit_distance_correct(self, text: str, vocabulary: Set[str], max_distance: int = 2) -> str:\n        words = text.split()\n        out = []\n        for w in words:\n            if w in vocabulary:\n                out.append(w)\n            else:\n                best = min(vocabulary, key=lambda v: textdistance.levenshtein(w, v))\n                if textdistance.levenshtein(w, best) <= max_distance:\n                    out.append(best)\n                else:\n                    out.append(w)\n        return \" \".join(out)\n\n    def frequency_correct(self, text: str) -> str:\n        words = text.split()\n        out = []\n        for w in words:\n            if w in self.word_freq:\n                out.append(w)\n            else:\n                candidates = [v for v in self.word_freq.keys() if textdistance.levenshtein(w, v) <= 2]\n                if candidates:\n                    best = max(candidates, key=lambda v: self.word_freq[v])\n                    out.append(best)\n                else:\n                    out.append(w)\n        return \" \".join(out)\n\n    @staticmethod\n    def regex_clean(text: str) -> str:\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n        return text\n\n\ndef compare_corrections(preds: List[str], targets: List[str], corrector: TextCorrector, vocabulary: Set[str]):\n    print(\"\\n\" + \"=\" * 80)\n    print(\"COMPARING ERROR-CORRECTION METHODS\")\n    print(\"=\" * 80)\n    methods = {\n        \"No correction\": preds,\n        \"SpellChecker\": [corrector.spell_check(p) for p in preds],\n        \"EditDistance\": [corrector.edit_distance_correct(p, vocabulary) for p in preds],\n        \"Frequency\": [corrector.frequency_correct(p) for p in preds],\n        \"Regex\": [TextCorrector.regex_clean(p) for p in preds],\n    }\n\n    results = {}\n    for name, corrected in methods.items():\n        w = wer(targets, corrected)\n        c = cer(targets, corrected)\n        results[name] = {\"WER\": w * 100, \"CER\": c * 100}\n        print(f\"\\n{name}:\")\n        print(f\"  WER: {w * 100:.2f}%\")\n        print(f\"  CER: {c * 100:.2f}%\")\n\n    best = min(results.items(), key=lambda kv: kv[1][\"WER\"])\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"Best method: {best[0]} (WER: {best[1]['WER']:.2f}%)\")\n    print(\"=\" * 80)\n    return results\n\n\ndef main():\n    print(\"=\" * 80)\n    print(\"DeepSpeech2-like training for LJSpeech (clean + English)\")\n    print(\"=\" * 80)\n    print(f\"Using device: {DEVICE}\")\n    print(f\"Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}, LR: {LEARNING_RATE}\")\n    print(\"-\" * 80)\n\n    dataset_path = download_ljspeech_if_missing(DATA_DIR)\n    df = load_ljspeech_metadata(dataset_path)\n\n    try:\n        from sklearn.model_selection import train_test_split\n    except Exception:\n        print(\"scikit-learn not available. Please install scikit-learn for train/test splitting.\")\n        sys.exit(1)\n\n    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n    print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\n    audio_proc = AudioProcessor(sr=SR, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n    text_transform = TextTransform()\n    print(f\"Vocabulary size (characters + blank): {text_transform.vocab_size()}\")\n\n    train_dataset = LJSpeechDataset(train_df, audio_proc, text_transform)\n    val_dataset = LJSpeechDataset(val_df, audio_proc, text_transform)\n    test_dataset = LJSpeechDataset(test_df, audio_proc, text_transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n    model = DeepSpeech2Like(n_mels=N_MELS, n_classes=text_transform.vocab_size(), n_rnn_layers=5, rnn_hidden=512, dropout=0.1)\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"Total model parameters: {total_params:,}\")\n\n    model = train_network(model, train_loader, val_loader, text_transform, epochs=EPOCHS, lr=LEARNING_RATE, device=DEVICE)\n\n    if os.path.exists(\"deepspeech2_best.pth\"):\n        model.load_state_dict(torch.load(\"deepspeech2_best.pth\", map_location=DEVICE))\n    else:\n        print(\"Warning: saved model not found (deepspeech2_best.pth). Using current model state for testing.\")\n\n    preds, targets, test_wer = test_network(model, test_loader, text_transform, device=DEVICE)\n\n    corrector = TextCorrector()\n    print(\"Training text corrector on train corpus...\")\n    corrector.fit_corpus(train_df[\"text\"].tolist())\n    vocab = set(\" \".join(train_df[\"text\"].tolist()).lower().split())\n\n    results = compare_corrections(preds, targets, corrector, vocab)\n\n    print(\"\\nAll done.\")\n    return model, results\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T21:03:07.497691Z","iopub.execute_input":"2025-11-25T21:03:07.497949Z","iopub.status.idle":"2025-11-25T22:49:08.324539Z","shell.execute_reply.started":"2025-11-25T21:03:07.497924Z","shell.execute_reply":"2025-11-25T22:49:08.323908Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDeepSpeech2-like training for LJSpeech (clean + English)\n================================================================================\nUsing device: cuda\nBatch size: 16, Epochs: 10, LR: 0.0003\n--------------------------------------------------------------------------------\nLoaded 13100 audio files from LJSpeech metadata.\nTrain: 10480, Val: 1310, Test: 1310\nVocabulary size (characters + blank): 29\nTotal model parameters: 27,060,541\nEpoch 1/10 - Batch 50/655 - Loss: 2.6738\nEpoch 1/10 - Batch 100/655 - Loss: 2.1685\nEpoch 1/10 - Batch 150/655 - Loss: 1.5646\nEpoch 1/10 - Batch 200/655 - Loss: 1.3653\nEpoch 1/10 - Batch 250/655 - Loss: 1.2534\nEpoch 1/10 - Batch 300/655 - Loss: 1.1794\nEpoch 1/10 - Batch 350/655 - Loss: 1.0234\nEpoch 1/10 - Batch 400/655 - Loss: 0.9839\nEpoch 1/10 - Batch 450/655 - Loss: 0.8419\nEpoch 1/10 - Batch 500/655 - Loss: 0.8476\nEpoch 1/10 - Batch 550/655 - Loss: 0.7784\nEpoch 1/10 - Batch 600/655 - Loss: 0.7958\nEpoch 1/10 - Batch 650/655 - Loss: 0.8190\n\nEpoch 1 summary:\n  Train Loss: 1.3495\n  Val   Loss: 0.6832\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 2/10 - Batch 50/655 - Loss: 0.6890\nEpoch 2/10 - Batch 100/655 - Loss: 0.6257\nEpoch 2/10 - Batch 150/655 - Loss: 0.6858\nEpoch 2/10 - Batch 200/655 - Loss: 0.6458\nEpoch 2/10 - Batch 250/655 - Loss: 0.5680\nEpoch 2/10 - Batch 300/655 - Loss: 0.6003\nEpoch 2/10 - Batch 350/655 - Loss: 0.5272\nEpoch 2/10 - Batch 400/655 - Loss: 0.5872\nEpoch 2/10 - Batch 450/655 - Loss: 0.5466\nEpoch 2/10 - Batch 500/655 - Loss: 0.4838\nEpoch 2/10 - Batch 550/655 - Loss: 0.5076\nEpoch 2/10 - Batch 600/655 - Loss: 0.5774\nEpoch 2/10 - Batch 650/655 - Loss: 0.4063\n\nEpoch 2 summary:\n  Train Loss: 0.5684\n  Val   Loss: 0.4469\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 3/10 - Batch 50/655 - Loss: 0.3917\nEpoch 3/10 - Batch 100/655 - Loss: 0.3362\nEpoch 3/10 - Batch 150/655 - Loss: 0.4094\nEpoch 3/10 - Batch 200/655 - Loss: 0.3070\nEpoch 3/10 - Batch 250/655 - Loss: 0.3049\nEpoch 3/10 - Batch 300/655 - Loss: 0.3625\nEpoch 3/10 - Batch 350/655 - Loss: 0.3478\nEpoch 3/10 - Batch 400/655 - Loss: 0.3063\nEpoch 3/10 - Batch 450/655 - Loss: 0.3387\nEpoch 3/10 - Batch 500/655 - Loss: 0.3290\nEpoch 3/10 - Batch 550/655 - Loss: 0.3733\nEpoch 3/10 - Batch 600/655 - Loss: 0.3149\nEpoch 3/10 - Batch 650/655 - Loss: 0.3018\n\nEpoch 3 summary:\n  Train Loss: 0.3796\n  Val   Loss: 0.3622\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 4/10 - Batch 50/655 - Loss: 0.2576\nEpoch 4/10 - Batch 100/655 - Loss: 0.2406\nEpoch 4/10 - Batch 150/655 - Loss: 0.2522\nEpoch 4/10 - Batch 200/655 - Loss: 0.8924\nEpoch 4/10 - Batch 250/655 - Loss: 0.3241\nEpoch 4/10 - Batch 300/655 - Loss: 0.2933\nEpoch 4/10 - Batch 350/655 - Loss: 0.3299\nEpoch 4/10 - Batch 400/655 - Loss: 0.2520\nEpoch 4/10 - Batch 450/655 - Loss: 0.2263\nEpoch 4/10 - Batch 500/655 - Loss: 0.1944\nEpoch 4/10 - Batch 550/655 - Loss: 0.2862\nEpoch 4/10 - Batch 600/655 - Loss: 0.2851\nEpoch 4/10 - Batch 650/655 - Loss: 0.2239\n\nEpoch 4 summary:\n  Train Loss: 0.2742\n  Val   Loss: 0.3104\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 5/10 - Batch 50/655 - Loss: 0.2044\nEpoch 5/10 - Batch 100/655 - Loss: 0.2713\nEpoch 5/10 - Batch 150/655 - Loss: 0.2736\nEpoch 5/10 - Batch 200/655 - Loss: 0.2283\nEpoch 5/10 - Batch 250/655 - Loss: 0.1592\nEpoch 5/10 - Batch 300/655 - Loss: 0.1997\nEpoch 5/10 - Batch 350/655 - Loss: 0.2196\nEpoch 5/10 - Batch 400/655 - Loss: 0.1954\nEpoch 5/10 - Batch 450/655 - Loss: 0.2274\nEpoch 5/10 - Batch 500/655 - Loss: 0.2562\nEpoch 5/10 - Batch 550/655 - Loss: 0.1912\nEpoch 5/10 - Batch 600/655 - Loss: 0.2092\nEpoch 5/10 - Batch 650/655 - Loss: 0.1831\n\nEpoch 5 summary:\n  Train Loss: 0.2121\n  Val   Loss: 0.2765\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 6/10 - Batch 50/655 - Loss: 0.2412\nEpoch 6/10 - Batch 100/655 - Loss: 0.1584\nEpoch 6/10 - Batch 150/655 - Loss: 0.1480\nEpoch 6/10 - Batch 200/655 - Loss: 0.1460\nEpoch 6/10 - Batch 250/655 - Loss: 0.0883\nEpoch 6/10 - Batch 300/655 - Loss: 0.1328\nEpoch 6/10 - Batch 350/655 - Loss: 0.1781\nEpoch 6/10 - Batch 400/655 - Loss: 0.1960\nEpoch 6/10 - Batch 450/655 - Loss: 0.2016\nEpoch 6/10 - Batch 500/655 - Loss: 0.1634\nEpoch 6/10 - Batch 550/655 - Loss: 0.1433\nEpoch 6/10 - Batch 600/655 - Loss: 0.1605\nEpoch 6/10 - Batch 650/655 - Loss: 0.2175\n\nEpoch 6 summary:\n  Train Loss: 0.1687\n  Val   Loss: 0.2632\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 7/10 - Batch 50/655 - Loss: 0.1016\nEpoch 7/10 - Batch 100/655 - Loss: 0.1282\nEpoch 7/10 - Batch 150/655 - Loss: 0.1305\nEpoch 7/10 - Batch 200/655 - Loss: 0.0915\nEpoch 7/10 - Batch 250/655 - Loss: 0.1145\nEpoch 7/10 - Batch 300/655 - Loss: 0.1131\nEpoch 7/10 - Batch 350/655 - Loss: 0.1517\nEpoch 7/10 - Batch 400/655 - Loss: 0.1115\nEpoch 7/10 - Batch 450/655 - Loss: 0.1529\nEpoch 7/10 - Batch 500/655 - Loss: 0.1734\nEpoch 7/10 - Batch 550/655 - Loss: 0.1354\nEpoch 7/10 - Batch 600/655 - Loss: 0.3540\nEpoch 7/10 - Batch 650/655 - Loss: 0.1378\n\nEpoch 7 summary:\n  Train Loss: 0.1436\n  Val   Loss: 0.2549\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 8/10 - Batch 50/655 - Loss: 0.1274\nEpoch 8/10 - Batch 100/655 - Loss: 0.1252\nEpoch 8/10 - Batch 150/655 - Loss: 0.1098\nEpoch 8/10 - Batch 200/655 - Loss: 0.0857\nEpoch 8/10 - Batch 250/655 - Loss: 0.1202\nEpoch 8/10 - Batch 300/655 - Loss: 0.1203\nEpoch 8/10 - Batch 350/655 - Loss: 0.1144\nEpoch 8/10 - Batch 400/655 - Loss: 0.1419\nEpoch 8/10 - Batch 450/655 - Loss: 0.1307\nEpoch 8/10 - Batch 500/655 - Loss: 0.1250\nEpoch 8/10 - Batch 550/655 - Loss: 0.1043\nEpoch 8/10 - Batch 600/655 - Loss: 0.1694\nEpoch 8/10 - Batch 650/655 - Loss: 0.1358\n\nEpoch 8 summary:\n  Train Loss: 0.1209\n  Val   Loss: 0.2618\n------------------------------------------------------------\nEpoch 9/10 - Batch 50/655 - Loss: 0.0831\nEpoch 9/10 - Batch 100/655 - Loss: 0.0721\nEpoch 9/10 - Batch 150/655 - Loss: 0.1030\nEpoch 9/10 - Batch 200/655 - Loss: 0.0991\nEpoch 9/10 - Batch 250/655 - Loss: 0.1426\nEpoch 9/10 - Batch 300/655 - Loss: 0.1026\nEpoch 9/10 - Batch 350/655 - Loss: 0.0774\nEpoch 9/10 - Batch 400/655 - Loss: 0.1040\nEpoch 9/10 - Batch 450/655 - Loss: 0.1216\nEpoch 9/10 - Batch 500/655 - Loss: 0.1053\nEpoch 9/10 - Batch 550/655 - Loss: 0.0794\nEpoch 9/10 - Batch 600/655 - Loss: 0.0805\nEpoch 9/10 - Batch 650/655 - Loss: 0.0949\n\nEpoch 9 summary:\n  Train Loss: 0.1073\n  Val   Loss: 0.2326\nSaved best model to deepspeech2_best.pth\n------------------------------------------------------------\nEpoch 10/10 - Batch 50/655 - Loss: 0.0884\nEpoch 10/10 - Batch 100/655 - Loss: 0.0678\nEpoch 10/10 - Batch 150/655 - Loss: 0.0651\nEpoch 10/10 - Batch 200/655 - Loss: 0.1057\nEpoch 10/10 - Batch 250/655 - Loss: 0.0836\nEpoch 10/10 - Batch 300/655 - Loss: 0.0926\nEpoch 10/10 - Batch 350/655 - Loss: 0.0819\nEpoch 10/10 - Batch 400/655 - Loss: 0.0907\nEpoch 10/10 - Batch 450/655 - Loss: 0.1567\nEpoch 10/10 - Batch 500/655 - Loss: 0.1182\nEpoch 10/10 - Batch 550/655 - Loss: 0.1324\nEpoch 10/10 - Batch 600/655 - Loss: 0.1180\nEpoch 10/10 - Batch 650/655 - Loss: 0.1197\n\nEpoch 10 summary:\n  Train Loss: 0.0906\n  Val   Loss: 0.2491\n------------------------------------------------------------\n\nTest results:\n  WER: 27.66%\n  CER: 6.62%\n\nExample predictions:\n\nTarget    : but oswald had rented post office box three zero zero six one in new orleans on june three nineteen sixtythree\nPrediction: but oswald had rented post office box three zero zero six one in new orlians on june three nineteen sixtythreeq\n\nTarget    : however there is no evidence that these men failed to take any action in dallas within their power that would have averted the tragedy\nPrediction: however there is noeividence that these men failed to take any action in dallas within their power that would have averted the tragedy as will be seen\n\nTarget    : at one time the marshalsea was the receptacle of pirates but none were committed to it after seventeen eightynine\nPrediction: at one time the marshal seen was the receptical of pirets but non were committed to it after seventeen eightynineq\n\nTarget    : remainder of motorcade\nPrediction: remainer of motorcadeq\n\nTarget    : and with all the appearances of spontaneity as locomotive bodies\nPrediction: and with all the apperances of spontoneety as locomotive bodingsq\nTraining text corrector on train corpus...\n\n================================================================================\nCOMPARING ERROR-CORRECTION METHODS\n================================================================================\n\nNo correction:\n  WER: 27.66%\n  CER: 6.62%\n\nSpellChecker:\n  WER: 18.43%\n  CER: 5.69%\n\nEditDistance:\n  WER: 22.92%\n  CER: 6.98%\n\nFrequency:\n  WER: 22.89%\n  CER: 8.22%\n\nRegex:\n  WER: 27.66%\n  CER: 6.62%\n\n================================================================================\nBest method: SpellChecker (WER: 18.43%)\n================================================================================\n\nAll done.\n","output_type":"stream"}],"execution_count":2}]}